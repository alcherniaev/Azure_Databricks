{"cells":[{"cell_type":"markdown","source":["#Perform date and time manipulation\n\n** Data Source **\n* English Wikipedia pageviews by second\n* Size on Disk: ~255 MB\n* Type: Parquet files\n* More Info: <a href=\"https://datahub.io/en/dataset/english-wikipedia-pageviews-by-second\" target=\"_blank\">https&#58;//datahub.io/en/dataset/english-wikipedia-pageviews-by-second</a>\n\n**Technical Accomplishments:**\n* Explore more of the `...sql.functions` operations\n  * Date & time functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91173802-6a49-4a37-8123-9a3e0f221c88"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f57d99dd-77e2-4200-a0d4-97cf825312fb"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41634e63-0747-4d1c-b099-ace3c1d5ac09"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) The Data Source\n\nThis data uses the **Pageviews By Seconds** data set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5e30287-4fe2-486b-93e0-27f04d52fc1c"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# I've already gone through the exercise to determine\n# how many partitions I want and in this case it is...\npartitions = 8\n\n# Make sure wide operations don't repartition to 200\nspark.conf.set(\"spark.sql.shuffle.partitions\", str(partitions))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b87d86fc-c4db-4b46-9173-71dc7ae3fc6b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["(source, sasEntity, sasToken) = getAzureDataSource()\nspark.conf.set(sasEntity, sasToken)\n\n# The directory containing our parquet files.\nparquetFile = source + \"/wikipedia/pageviews/pageviews_by_second.parquet/\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a504b66-b38e-43d6-91ad-db969657d2b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create our initial DataFrame. We can let it infer the \n# schema because the cost for parquet files is really low.\ninitialDF = (spark.read\n  .option(\"inferSchema\", \"true\") # The default, but not costly w/Parquet\n  .parquet(parquetFile)          # Read the data in\n  .repartition(partitions)       # From 7 >>> 8 partitions\n  .cache()                       # Cache the expensive operation\n)\n# materialize the cache\ninitialDF.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7ebebe0-48b9-4433-b548-65f3ed523e38"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Preparing Our Data\n\nIf we will be working on any given dataset for a while, there are a handful of \"necessary\" steps to get us ready...\n\nMost of which we've just knocked out above.\n\n**Basic Steps**\n0. <div style=\"text-decoration:line-through\">Read the data in</div>\n0. <div style=\"text-decoration:line-through\">Balance the number of partitions to the number of slots</div>\n0. <div style=\"text-decoration:line-through\">Cache the data</div>\n0. <div style=\"text-decoration:line-through\">Adjust the `spark.sql.shuffle.partitions`</div>\n0. Perform some basic ETL (i.e., convert strings to timestamp)\n0. Possibly re-cache the data if the ETL was costly\n\nWhat we haven't done is some of the basic ETL necessary to explore our data.\n\nNamely, the problem is that the field \"timestamp\" is a string.\n\nIn order to performed date/time - based computation I need to convert this to an alternate datetime format."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bede602-e27e-430c-9ab5-fa9f00665c0f"}}},{"cell_type":"code","source":["initialDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56754873-4ef5-4429-ae2a-fd16f39e116a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) withColumnRenamed(..), withColumn(..), select(..)\n\nMy first hangup is that we have a **column named timestamp** and the **datatype will also be timestamp**\n\nThe nice thing about Apache Spark is that I'm allowed the have an issue with this because it's very easy to fix...\n\nJust rename the column..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc923f3b-9032-4dcd-9e71-f55e1a493f9a"}}},{"cell_type":"code","source":["(initialDF\n  .select( col(\"timestamp\").alias(\"capturedAt\"), col(\"site\"), col(\"requests\") )\n  .printSchema()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8457104-8341-4d2c-9b5d-83f09f4ab49e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["There are a number of different ways to rename a column..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e6b4ff7-d93c-4521-af18-1cb8a7a668e7"}}},{"cell_type":"code","source":["(initialDF\n  .withColumnRenamed(\"timestamp\", \"capturedAt\")\n  .printSchema()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8219347d-7b5f-43b9-adb5-c7a19d1eb1a2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["(initialDF\n  .toDF(\"capturedAt\", \"site\", \"requests\")\n  .printSchema()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d076d077-51fb-4080-86fe-29d228d170ec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) unix_timestamp(..) & cast(..)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"453e4199-552c-4918-860c-40201730e32f"}}},{"cell_type":"markdown","source":["Now that **we** are over **my** hangup, we can focus on converting the **string** to a **timestamp**.\n\nFor this we will be looking at more of the functions in the `functions` package\n* `pyspark.sql.functions` in the case of Python\n* `org.apache.spark.sql.functions` in the case of Scala & Java\n\nAnd so that we can watch the transformation, will will take one step at a time..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3b270ff-ea55-487d-a13e-ac28a9b59f7c"}}},{"cell_type":"markdown","source":["The first function is `unix_timestamp(..)`\n\nIf you look at the API docs, `unix_timestamp(..)` is described like this:\n> Convert time string with given pattern (see <a href=\"http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html\" target=\"_blank\">SimpleDateFormat</a>) to Unix time stamp (in seconds), return null if fail.\n\n`SimpleDataFormat` is part of the Java API and provides support for parsing and formatting date and time values.\n\nIn order to know what format the data is in, let's take a look at the first row..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11d0c9c1-47e7-4cd3-bbb0-c5ad7b8cba02"}}},{"cell_type":"markdown","source":["Comparing that value with the patterns express in the docs for the `SimpleDateFormat` class, we can come up with a format:\n\n**yyyy-MM-dd HH:mm:ss**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a71af41-dc2c-4982-8523-abc2e55959fa"}}},{"cell_type":"code","source":["tempA = (initialDF\n  .withColumnRenamed(\"timestamp\", \"capturedAt\")\n  .select( col(\"*\"), unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd HH:mm:ss\") )\n)\ntempA.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f648c1d-3577-458a-bb36-5a04dd4275a6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(tempA)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50d6bb3e-cda5-4b87-94a8-072c40b65549"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["** *Note:* ** *If you haven't caught it yet, there is a bug in the previous code....*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17dccbc1-a0de-4ca6-a311-15bdcf4e74d3"}}},{"cell_type":"markdown","source":["A couple of things happened...\n0. We ended up with a new column - that's OK for now\n0. The new column has a really funky name - based upon the name of the function we called and its parameters.\n0. The data type is now a long.\n  * This value is the Java Epoch\n  * The number of seconds since 1970-01-01T00:00:00Z\n  \nWe can now take that epoch value and use the `Column.cast(..)` method to convert it to a **timestamp**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c72446a-3833-489e-a26d-889f22705427"}}},{"cell_type":"code","source":["tempB = (initialDF\n  .withColumnRenamed(\"timestamp\", \"capturedAt\")\n  .select( col(\"*\"), unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\n)\ntempB.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb2cf291-072a-4ecf-b466-be6fbbc6c79a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(tempB)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed25d328-a2d3-4be7-9b5c-94668b0f8fbd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now that our column `createdAt` has been converted from a **string** to a **timestamp**, we just need to deal with this REALLY funky column name.\n\nAgain.. there are several ways to do this.\n\nI'll let you decide which you like better..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7e4e76c-b5d5-4a0c-ad28-4dbb97c48eca"}}},{"cell_type":"markdown","source":["### Option #1\nThe `as()` or `alias()` method can be appended to the chain of calls.\n\nThis version will actually produce an odd little bug.<br/>\nThat is, how do you get rid of only one of the two `capturedAt` columns?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b390d09-2e18-4a0e-93be-251f3ef00183"}}},{"cell_type":"code","source":["tempC = (initialDF\n  .withColumnRenamed(\"timestamp\", \"capturedAt\")\n  .select( col(\"*\"), unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\").alias(\"capturedAt\") )\n)\ntempC.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7532eeb4-752d-452a-b030-6e186817d51e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(tempC)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c6e467f-1318-4c32-abec-689ce4870d20"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Option #2\nThe `withColumn(..)` renames the column (first param) and accepts as a<br/>\nsecond parameter the expression(s) we need for our transformation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73df340e-b3c7-4e80-83f7-7528ab0072a5"}}},{"cell_type":"code","source":["tempD = (initialDF\n  .withColumnRenamed(\"timestamp\", \"capturedAt\")\n  .withColumn(\"capturedAt\", unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\n)\ntempD.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84337ec6-e8d6-4ed7-847a-c27612f522a8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(tempD)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80154bf7-863c-4dc8-86e9-891d9d99312a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Option #3\n\nWe can take the big ugly name explicitly rename it.\n\nThis version will actually produce an odd little bug.<br/>\nThat is how do you get rid of only one of the two \"capturedAt\" columns?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81ec81e1-d7cf-4a6f-9ec1-8bceeb8d7bcf"}}},{"cell_type":"code","source":["#Option #3\n\ntempE = (initialDF\n  .withColumnRenamed(\"timestamp\", \"capturedAt\")\n  .select( col(\"*\"), unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\n  .withColumnRenamed(\"CAST(unix_timestamp(capturedAt, yyyy-MM-dd'T'HH:mm:ss) AS TIMESTAMP)\", \"capturedAt\")\n  # .drop(\"timestamp\")\n)\ntempE.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66cc9707-ea1a-4ec3-8f5a-7d2226b074c7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(tempE)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d47caa9-7ab6-4bae-b449-81ffe1d1af4d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Option #4\n\nThe last version is a twist on the others in which we start with the <br/>\nname `timestamp` and rename it and the expression all in one call<br/>\n\nBut this version leaves us with the old column in the DF"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bcb7bbff-efc3-4fbc-a444-a3b20de043a0"}}},{"cell_type":"code","source":["tempF = (initialDF\n  .withColumn(\"capturedAt\", unix_timestamp( col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\n)\ntempF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d428e597-f47d-4f39-985a-a3e75ec9957f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(tempF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfd66ee0-0320-4dba-9d81-761522e745d9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Let's pick the \"cleanest\" version...\n\nAnd with our base `DataFrame` in place we can start exploring the data a little..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c802d3a5-f796-4761-a7ba-30e8429149dc"}}},{"cell_type":"code","source":["pageviewsDF = (initialDF\n  .withColumnRenamed(\"timestamp\", \"capturedAt\")\n  .withColumn(\"capturedAt\", unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\n)\n\npageviewsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b566ba2-84a2-41ec-a094-25f36a4c4662"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(pageviewsDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0053e1f4-7140-4b4a-bdc3-32aee474632c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["And just so that we don't have to keep performing these transformations.... \n\nMark the `DataFrame` as cached and then materialize the result."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a462e7ab-f04a-40d1-9186-8b79bf58d809"}}},{"cell_type":"code","source":["pageviewsDF.cache().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15d9d69c-17e8-47bf-a436-9ece8b7f604f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) year(..), month(..), dayofyear(..)\n\nLet's take a look at some of the other date & time functions...\n\nWith that we can answer a simple question: When was this data captured.\n\nWe can start specifically with the year..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af06ebd1-e3ac-4296-a7d2-0452c74462cd"}}},{"cell_type":"code","source":["display(\n  pageviewsDF\n    .select( year( col(\"capturedAt\")) ) # Every record converted to a single column - the year captured\n    .distinct()                         # Reduce all years to the list of distinct years\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24291992-4690-49e4-9936-2b7255baf6be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now let's take a look at in which months was this data captured..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49c5834e-89b9-47a7-9eb8-a7b9f071a66c"}}},{"cell_type":"code","source":["display(\n  pageviewsDF\n    .select( month( col(\"capturedAt\")) ) # Every record converted to a single column - the month captured\n    .distinct()                          # Reduce all months to the list of distinct months\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffe3a9ef-8d9a-4519-8780-644c1a10a6f6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["And of course this both can be combined as a single call..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6113ab5a-9091-4dc8-bad3-07b8e7d355b9"}}},{"cell_type":"code","source":["(pageviewsDF\n  .select( month(col(\"capturedAt\")).alias(\"month\"), year(col(\"capturedAt\")).alias(\"year\"))\n  .distinct()\n  .show()                     \n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45722a59-6286-42fe-a0c3-3e9ee34780e5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["It's pretty easy to see that the data was captured during March & April of 2015.\n\nWe will have more opportunities to play with the various date and time functions in the next exercise.\n\nFor now, let's just make sure to review them in the Spark API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64b84430-e38c-4ac8-9d7e-a0bf526f41ea"}}},{"cell_type":"markdown","source":["## Next steps\n\nStart the next lesson, [Use aggregate functions]($./2.Use-Aggregate-Functions)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"142d6194-c5fd-4ff8-9e6c-5cf63a4cccca"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1.DateTime-Manipulation","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1522049303757538}},"nbformat":4,"nbformat_minor":0}
