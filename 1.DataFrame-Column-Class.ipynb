{"cells":[{"cell_type":"markdown","source":["#DataFrame Column Class\n\n** Data Source **\n* One hour of Pagecounts from the English Wikimedia projects captured August 5, 2016, at 12:00 PM UTC.\n* Size on Disk: ~23 MB\n* Type: Compressed Parquet File\n* More Info: <a href=\"https://dumps.wikimedia.org/other/pagecounts-raw\" target=\"_blank\">Page view statistics for Wikimedia projects</a>\n\n**Technical Accomplishments:**\n* Continue exploring the `DataFrame` set of APIs.\n* Introduce the `Column` class"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a920e814-8151-4fb3-bde0-e41d11ea0d8a"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f47adb3c-f864-4204-b3ac-f5b94012d6d2"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb166efe-b1dd-4dc7-8786-614b722a9c6a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) **The Data Source**\n\nWe will be using the same data source as our previous notebook.\n\nAs such, we can go ahead and start by creating our initial `DataFrame`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5c01348-556e-4158-99b2-01af569a4f6a"}}},{"cell_type":"code","source":["(source, sasEntity, sasToken) = getAzureDataSource()\nspark.conf.set(sasEntity, sasToken)\n\nparquetFile = source + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05af237c-0a50-4d3b-bb56-7fb850c53c60"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["pagecountsEnAllDF = (spark  # Our SparkSession & Entry Point\n  .read                     # Our DataFrameReader\n  .parquet(parquetFile)     # Returns an instance of DataFrame\n  .cache()                  # cache the data\n)\nprint(pagecountsEnAllDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f9129e8-62b7-4512-ba49-7bee0d5136b4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's take another look at the number of records in our `DataFrame`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52987adb-fd6c-4437-a5ed-e85b42a8c8cd"}}},{"cell_type":"code","source":["total = pagecountsEnAllDF.count()\n\nprint(\"Record Count: {0:,}\".format( total ))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fd3e649-781d-40d5-85d7-66827ad4f419"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now let's take another peek at our data..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02b8ec9f-12a7-459c-8ba3-4a0ffb62c597"}}},{"cell_type":"code","source":["display(pagecountsEnAllDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bfdf817-350c-496b-84bc-5a1ea52e7d33"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As we view the data, we can see that there is no real rhyme or reason as to how the data is sorted.\n* We cannot even tell if the column **project** is sorted - we are seeing only the first 1,000 of some 2.3 million records.\n* The column **article** is not sorted as evident by the article **A_Little_Boy_Lost** appearing between a bunch of articles starting with numbers and symbols.\n* The column **requests** is clearly not sorted.\n* And our **bytes_served** contains nothing but zeros.\n\nSo let's start by sorting our data. In doing this, we can answer the following question:\n\nWhat are the top 10 most requested articles?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b637d7e-8a71-4f30-88d5-c505303025e6"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) orderBy(..) & sort(..)\n\nIf you look at the API docs, `orderBy(..)` is described like this:\n> Returns a new Dataset sorted by the given expressions.\n\nBoth `orderBy(..)` and `sort(..)` arrange all the records in the `DataFrame` as specified.\n* Like `distinct()` and `dropDuplicates()`, `sort(..)` and `orderBy(..)` are aliases for each other.\n  * `sort(..)` appealing to functional programmers.\n  * `orderBy(..)` appealing to developers with an SQL background.\n* Like `orderBy(..)` there are two variants of these two methods:\n  * `orderBy(Column)`\n  * `orderBy(String)`\n  * `sort(Column)`\n  * `sort(String)`\n\nAll we need to do now is sort our previous `DataFrame`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"923160f9-adfe-4d00-9053-64f46afbcfd3"}}},{"cell_type":"code","source":["sortedDF = (pagecountsEnAllDF\n  .orderBy(\"requests\")\n)\nsortedDF.show(10, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fc9ae8c-0300-47da-ac40-a86b83275539"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As you can see, we are not sorting correctly.\n\nWe need to reverse the sort.\n\nOne might conclude that we could make a call like this:\n\n`pagecountsEnAllDF.orderBy(\"requests desc\")`\n\nTry it in the cell below:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37944aa5-3613-46f3-826c-813fd3f605c3"}}},{"cell_type":"code","source":["# Uncomment and try this:\n# pagecountsEnAllDF.orderBy(\"requests desc\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6872ae9d-1939-4c62-b6fc-2e126b4c5719"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Why does this not work?\n* The `DataFrames` API is built upon an SQL engine.\n* There is a lot of familiarity with this API and SQL syntax in general.\n* The problem is that `orderBy(..)` expects the name of the column.\n* What we specified was an SQL expression in the form of **requests desc**.\n* What we need is a way to programmatically express such an expression.\n* This leads us to the second variant, `orderBy(Column)` and more specifically, the class `Column`.\n\n** *Note:* ** *Some of the calls in the `DataFrames` API actually accept SQL expressions.*<br/>\n*While these functions will appear in the docs as `someFunc(String)` it's very*<br>\n*important to thoroughly read and understand what the parameter actually represents.*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b76ac9ae-f561-43df-a961-826f6301b3f5"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) The Column Class\n\nThe `Column` class is an object that encompasses more than just the name of the column, but also column-level-transformations, such as sorting in a descending order.\n\nThe first question to ask is how do I create a `Column` object?\n\nIn Scala we have these options:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fe09283-ac08-42ab-beaf-54282edf62d1"}}},{"cell_type":"markdown","source":["** *Note:* ** *We are showing both the Scala and Python versions below for comparison.*<br/>\n*Make sure to run only the one cell for your notebook's default language (Scala or Python)*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a3a7752-db44-4abc-8fd6-d54bffee1cbe"}}},{"cell_type":"code","source":["%scala\n\n// Scala & Python both support accessing a column from a known DataFrame\n// Uncomment this if you are using the Scala version of this notebook\n// val columnA = pagecountsEnAllDF(\"requests\")    \n\n// This option is Scala specific, but is arugably the cleanest and easy to read.\nval columnB = $\"requests\"          \n\n// If we import ...sql.functions, we get a couple of more options:\nimport org.apache.spark.sql.functions._\n\n// This uses the col(..) function\nval columnC = col(\"requests\")\n\n// This uses the expr(..) function which parses an SQL Expression\nval columnD = expr(\"a + 1\")\n\n// This uses the lit(..) to create a literal (constant) value.\nval columnE = lit(\"abc\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0b59095-06fa-4177-89dd-a41b3c64ca2f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In Python we have these options:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0e26f1c-643b-41e2-8857-affc37a7c709"}}},{"cell_type":"code","source":["%python\n\n# Scala & Python both support accessing a column from a known DataFrame\n# Uncomment this if you are using the Python version of this notebook\n# columnA = pagecountsEnAllDF[\"requests\"]\n\n# The $\"column-name\" version that works for Scala does not work in Python\n# columnB = $\"requests\"      \n\n# If we import ...sql.functions, we get a couple of more options:\nfrom pyspark.sql.functions import *\n\n# This uses the col(..) function\ncolumnC = col(\"requests\")\n\n# This uses the expr(..) function which parses an SQL Expression\ncolumnD = expr(\"a + 1\")\n\n# This uses the lit(..) to create a literal (constant) value.\ncolumnE = lit(\"abc\")\n\n# Print the type of each attribute\nprint(\"columnC: {}\".format(columnC))\nprint(\"columnD: {}\".format(columnD))\nprint(\"columnE: {}\".format(columnE))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df2af883-83c1-4448-8626-b47fef94d2f2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the case of Scala, the cleanest version is the **$\"column-name\"** variant.\n\nIn the case of Python, the cleanest version is the **col(\"column-name\")** variant.\n\nSo with that, we can now create a `Column` object, and apply the `desc()` operation to it:\n\n** *Note:* ** *We are introducing `...sql.functions` specifically for creating `Column` objects.*<br/>\n*We will be reviewing the multitude of other commands available from this part of the API in future notebooks.*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a0702be-47e3-4bac-aea1-3aa3cf49f1b7"}}},{"cell_type":"code","source":["column = col(\"requests\").desc()\n\n# Print the column type\nprint(\"column:\", column)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c54e064-9313-4b86-b687-596072011609"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And now we can piece it all together..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3583c766-ce14-4051-835b-698e97f88fef"}}},{"cell_type":"code","source":["sortedDescDF = (pagecountsEnAllDF\n  .orderBy( col(\"requests\").desc() )\n)  \nsortedDescDF.show(10, False) # The top 10 is good enough for now"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3a6fef7-c3b6-4951-a839-4eb5177e8d71"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["It should be of no surprise that the **Main_Page** (in both the Wikipedia and Wikimedia projects) is the most requested page.\n\nFollowed shortly after that is **Special:Search**, Wikipedia's search page.\n\nAnd if you consider that this data was captured in the August before the 2016 presidential election, the Trumps will be one of the most requested pages on Wikipedia."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2faff71b-fde8-48e2-8a12-cac6c3d67285"}}},{"cell_type":"markdown","source":["### Review Column Class\n\nThe `Column` objects provide us a programmatic way to build up SQL-ish expressions.\n\nBesides the `Column.desc()` operation we used above, we have a number of other operations that can be performed on a `Column` object.\n\nHere is a preview of the various functions - we will cover many of these as we progress through the class:\n\n**Column Functions**\n* Various mathematical functions such as add, subtract, multiply & divide\n* Various bitwise operators such as AND, OR & XOR\n* Various null tests such as `isNull()`, `isNotNull()` & `isNaN()`.\n* `as(..)`, `alias(..)` & `name(..)` - Returns this column aliased with a new name or names (in the case of expressions that return more than one column, such as explode).\n* `between(..)` - A boolean expression that is evaluated to true if the value of this expression is between the given columns.\n* `cast(..)` & `astype(..)` - Convert the column into type dataType.\n* `asc(..)` - Returns a sort expression based on the ascending order of the given column name.\n* `desc(..)` - Returns a sort expression based on the descending order of the given column name.\n* `startswith(..)` - String starts with.\n* `endswith(..)` - String ends with another string literal.\n* `isin(..)` - A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments.\n* `like(..)` - SQL like expression\n* `rlike(..)` - SQL RLIKE expression (LIKE with Regex).\n* `substr(..)` - An expression that returns a substring.\n* `when(..)` & `otherwise(..)` - Evaluates a list of conditions and returns one of multiple possible result expressions.\n\nThe complete list of functions differs from language to language."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcc8a54f-64e0-48e5-8094-0b68f648bb47"}}},{"cell_type":"markdown","source":["## Next steps\n\nStart the next lesson, [Work with Column expressions]($./2.DataFrame-Column-Expressions)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8fb91fa-6351-40df-92e7-17a8a148f2a3"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1.DataFrame-Column-Class","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1549433775244214}},"nbformat":4,"nbformat_minor":0}
